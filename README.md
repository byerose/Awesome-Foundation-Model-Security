> üö© Recently, there have been significant advances in generative models, including the diffusion model in the field of image processing, large language models in the field of text processing, and even diffusion models that have been extended to speech and video processing. The rapid development of generative models has forced us to consider their security issues. This repository primarily focuses on the security issues arising from generative models and how to use generative models to solve existing security problems.

# Table of Contents
- [Paper list](#Paper-list)
  - [Survey](#survey)
  - [Risks of Model](#Risks-of-Model)
    - [Prompt Injection](#Prompt-Injection)
    - [Poisoning](#Poisoning)
    - [Privacy](#Privacy)
  - [Evaluation on Content](#Evaluation-on-Content)
- [Workshop](#Workshop)

# Paper list

## üìåSurvey

### Representative Model
- Improved Denoising Diffusion Probabilistic Models [[ICML '21]](https://arxiv.org/abs/2102.09672) [[code]](https://github.com/openai/improved-diffusion)
- InCoder: A Generative Model for Code Infilling and Synthesis [[ICLR '23]](http://arxiv.org/abs/2204.05999) [[code]](https://github.com/dpfried/incoder)
- CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis [[ICLR '23]](https://arxiv.org/abs/2203.13474) [[code]](https://github.com/salesforce/CodeGen)

## üßØRisks of Model

### Prompt Injection

- More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models [[ArXiv '23]](https://arxiv.org/abs/2302.12173) [[code]](https://github.com/greshake/lm-safety)
  <details><summary></summary>

### Poisoning

- BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT [[NDSS '23 Poster]](https://arxiv.org/abs/2304.12298) 
  <details><summary></summary>

- Analyzing And Editing Inner Mechanisms of Backdoored Language Models [[ArXiv '23]](http://arxiv.org/abs/2302.12461)
  <details><summary></summary>

### Privacy
- Multi-step Jailbreaking Privacy Attacks on ChatGPT [[ArXiv '23]](http://arxiv.org/abs/2304.05197)
  <details><summary></summary> 
- Extracting Training Data from Large Language Models [[USENIX '21]](https://arxiv.org/abs/2012.07805) [[code]](https://github.com/ftramer/LM_Memorization)
  <details><summary></summary>

## üéÑEvaluation on Content
- LEVER: Learning to Verify Language-to-Code Generation with Execution [[ArXiv]](https://arxiv.org/abs/2302.08468) [[code]](https://github.com/niansong1996/lever)
  <details><summary></summary> 
- Holistic Evaluation of Language Models [[ArXiv '22]](https://arxiv.org/abs/2211.09110) [[code]](https://github.com/stanford-crfm/helm) [[project]](https://crfm.stanford.edu/helm/latest/)
  <details><summary></summary>

## üóúÔ∏èModel Application

### Vision

### language



# Workshop
- Challenges of Deploying Generative AI [[ICML '23]](https://deployinggenerativeai.github.io/index)
