> introducution

# Table of Contents
- [Paper list](#Paper-list)
  - [Survey](#survey)
  - [Risks of Model](#Risks-of-Model)
    - [Prompt Injection](#Prompt-Injection)
    - [Poisoning](#Poisoning)
    - [Privacy](#Privacy)
  - [Evaluation on Content](#Evaluation-on-Content)
- [Workshop](#Workshop)

# Paper list

## ðŸ“ŒSurvey

### Representative Model

- InCoder: A Generative Model for Code Infilling and Synthesis [[ICLR 2023]](http://arxiv.org/abs/2204.05999) [[code]](https://github.com/dpfried/incoder)
  <details><summary></summary>

- CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis [[ArXiv]](https://arxiv.org/abs/2203.13474) [[code]](https://github.com/salesforce/CodeGen)
  <details><summary></summary>

## ðŸ§¯Risks of Model

### Prompt Injection

- More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models [[ArXiv]](https://arxiv.org/abs/2302.12173) [[code]](https://github.com/greshake/lm-safety)
  <details><summary></summary>

### Poisoning

- BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT [[NDSS Poster]](https://arxiv.org/abs/2304.12298) 
  <details><summary></summary>

- Analyzing And Editing Inner Mechanisms of Backdoored Language Models [[ArXiv]](http://arxiv.org/abs/2302.12461)
  <details><summary></summary>

### Privacy
- Multi-step Jailbreaking Privacy Attacks on ChatGPT [[ArXiv]](http://arxiv.org/abs/2304.05197)
  <details><summary></summary> 

## ðŸŽ„Evaluation on Content
- LEVER: Learning to Verify Language-to-Code Generation with Execution [[ArXiv]](https://arxiv.org/abs/2302.08468) [[code]](https://github.com/niansong1996/lever)
  <details><summary></summary>

# Workshop
- Challenges of Deploying Generative AI [[ICML2023]](https://deployinggenerativeai.github.io/index)
