> introducution

# Table of Contents
- [Paper list](#Paper-list)
  - [Survey](#survey)
  - [Risks of Model](#Risks-of-Model)
    - [Poisoning](#Poisoning)
    - [Privacy](#Privacy)
  - [Evaluation on Content](#Evaluation-on-Content)
- [Workshop](#Workshop)

# Paper list

## Survey

## Risks of Model

### Prompt Injection
- More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models [[ArXiv]](https://arxiv.org/abs/2302.12173) [[code]](https://github.com/greshake/lm-safety)

### Poisoning
- BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT [[NDSS Poster]](https://arxiv.org/abs/2304.12298)
- Analyzing And Editing Inner Mechanisms of Backdoored Language Models [[ArXiv]](http://arxiv.org/abs/2302.12461)

### Privacy
- Multi-step Jailbreaking Privacy Attacks on ChatGPT [[ArXiv]](http://arxiv.org/abs/2304.05197)

## Evaluation on Content
- LEVER: Learning to Verify Language-to-Code Generation with Execution [[ArXiv]](https://arxiv.org/abs/2302.08468) [[code]](https://github.com/niansong1996/lever)

# Workshop
- Challenges of Deploying Generative AI [[ICML2023]](https://deployinggenerativeai.github.io/index)
